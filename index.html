<!DOCTYPE html>
<html lang="en-us">
<head>
    <meta charset="UTF-8">
    <title>Understanding Visual Concepts with Continuation Learning</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
</head>
<body>
    <section class="page-header">
        <h1 class="project-name">Understanding Visual Concepts with Continuation Learning</h1>
        <h2 class="project-tagline"></h2>
        <a href="https://github.com/willwhitney/unsupervised-dcign" class="btn">View on GitHub</a>
        <a href="https://github.com/willwhitney/unsupervised-dcign/zipball/master" class="btn">Download .zip</a>
        <a href="https://github.com/willwhitney/unsupervised-dcign/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
        <h1>
            <a id="abstract" class="anchor" href="#abstract" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>
            Abstract
        </h1>

        <p>We introduce a neural network architecture and a learning algorithm to produce factorized symbolic representations. We propose to learn these concepts by observing consecutive frames, letting all the components of the hidden representation except a small discrete set (gating units) be predicted from the previous frame, and let the factors of variation in the next frame be represented entirely by these discrete gated units. The model thus learns binary-valued gatings which correspond to symbolic representations. We demonstrate the efficacy of our approach on datasets of faces undergoing 3D transformations and Atari 2600 games.</p>

        <p>You can read the full ICLR submission <a href="http://beta.openreview.net/forum?id=r8lrDJ89Pf8wknpYt5zq">on OpenReview.</a></p>

        <h1><a id="results" class="anchor" href="#results" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>
            Results
        </h1>

        <p>These animations are generated by encoding one input image, then varying the value of a single component of this encoded representation before rendering the representation with the decoder. Each animation thus demonstrates the meaning of a single hidden unit of the autoencoder.</p>

        <p>Our first dataset is frames from Atari 2600 games. Just by watching video of gameplay, our model learns cleanly disentangled representations of the position of the paddle and the number of lives remaining in Breakout. In Space Invaders, it learns to represent the state of the aliens with a single unit, including the animations they make as they move across the screen. Though the renderings here are from one input frame each, these transformations work equally well with any game state; the model learns to symbolically represent these latent factors of variation from raw pixels.</p>

        <div class="image-row">
            <span class="image-wrapper"><img src="assets/moving_paddle/animated.gif">Moving the paddle.</span>
            <span class="image-wrapper"><img src="assets/counting_lives/animated.gif">Counting remaining lives.</span>
            <span class="image-wrapper"><img src="assets/moving_invaders_left_full_screen/animated.gif">Animating the aliens.</span>
        </div>

        <p>This dataset consists of rendered images of faces as they move up and down or left and right, and as the light source moves around them. The model is able to learn to infer the pose and lighting of these faces completely unsupervised; additionally, it can re-render the input face with a different pose or lighting. This result is comparable to the <a href="http://willwhitney.github.io/dc-ign/www/">DC-IGN</a>, but needs no supervision.</p>

        <div class="image-row">
            <span class="image-wrapper"><img src="assets/face_light/animated.gif"></span>
            <span class="image-wrapper"><img src="assets/face_elevation/animated.gif"></span>
            <span class="image-wrapper"><img src="assets/face_azimuth/animated.gif"></span>
        </div>

        <h1>
            <a id="code" class="anchor" href="#code" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>
            Code
        </h1>

        <p>All the code for this project is available at <a href="https://github.com/willwhitney/understanding-visual-concepts">https://github.com/willwhitney/understanding-visual-concepts</a>. It's very much under active development, so use it at your own risk.</p>

        <footer class="site-footer">
            <span class="site-footer-owner"><a href="https://github.com/willwhitney/unsupervised-dcign">Understanding Visual Concepts with Continuation Learning</a> is maintained by <a href="https://github.com/willwhitney">willwhitney</a>.</span>

            <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
        </footer>

        </section>


    </body>
    </html>
